{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from clipper_admin import ClipperConnection, DockerContainerManager\n",
    "from clipper_admin.deployers import python as python_deployer\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import signal\n",
    "import sys\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "query_queue = multiprocessing.Queue()\n",
    "clipper_conn = None\n",
    "\n",
    "def poisson_query_schedule_generator(query_rate, num_samples):\n",
    "    '''\n",
    "        Generator version of `genPoissonQuerySchedule` that returns\n",
    "        only single schedule tuples of (number of queries, time to sleep (ms)).\n",
    "    '''\n",
    "    num_samples = int(1.25*num_samples)\n",
    "    samples = np.random.poisson(query_rate, num_samples)\n",
    "    num_queries = 0\n",
    "    count = 0\n",
    "    for sample in samples:\n",
    "        if sample != 0:\n",
    "            if count != 0:\n",
    "                yield (0, count)\n",
    "                count = 0\n",
    "            yield (sample, 1)\n",
    "            num_queries += sample\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    if count != 0:\n",
    "        yield (0, count)\n",
    "\n",
    "def gen_poisson_query_schedule(query_rate, num_samples):\n",
    "    '''\n",
    "        Generates schedule of queries based on poisson arrival.\n",
    "        Schedule is a list of tuples of (number of queries, time to sleep (ms)).\n",
    "        \n",
    "        Args:\n",
    "            queryRate: Number of queries per second in milliseconds\n",
    "            numSamples: Number of samples to generate a schedule for\n",
    "            \n",
    "        Returns:\n",
    "            A tuple of (`actual number of samples`,`query schedule`).\n",
    "            `actual number of samples`: total number of samples in schedule\n",
    "                including buffer in case query count falls short.\n",
    "            `query schedule`: a list of tuples of (number of queries, time to sleep (ms))\n",
    "    '''\n",
    "    num_samples = int(1.25*num_samples)\n",
    "    samples = np.random.poisson(query_rate, num_samples)\n",
    "    \n",
    "    def fold_sample(state, sample):\n",
    "        schedule, num_queries, count = state\n",
    "        if sample != 0:\n",
    "            if count != 0:\n",
    "                schedule.append([0, count])\n",
    "                count = 0\n",
    "            schedule.append([sample, 1])\n",
    "            num_queries += sample\n",
    "            return (schedule, num_queries, count)\n",
    "        else:\n",
    "            return (schedule, num_queries, count + 1)\n",
    "    query_schedule, actual_num_queries, rem_count = reduce(fold_sample, samples, ([], 0, 0))\n",
    "    if rem_count != 0:\n",
    "        query_schedule.append([0, count])\n",
    "        \n",
    "    return actual_num_queries, query_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-15-4f75d836041b>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-4f75d836041b>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    def generate_queries(model_name, query_generator, on_exception=lambda e : print(e), query_rate, schedule=None, num_samples=MAX_SAMPLES):\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "MAX_SAMPLES = 100000\n",
    "\n",
    "def predict(addr, model_name, x, batch=False):\n",
    "    url = \"http://%s/%s/predict\" % addr % model_name\n",
    "\n",
    "    if batch:\n",
    "        req_json = json.dumps({'input_batch': x})\n",
    "    else:\n",
    "        req_json = json.dumps({'input': list(x)})\n",
    "\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    start = datetime.now()\n",
    "    r = requests.post(url, headers=headers, data=req_json)\n",
    "    end = datetime.now()\n",
    "    latency = (end - start).total_seconds() * 1000.0\n",
    "    print(\"'%s', %f ms\" % (r.text, latency))\n",
    "\n",
    "def query_worker(queue):\n",
    "    print(os.getpid(),\"working\")\n",
    "    while True:\n",
    "        item = Queue.get(True)\n",
    "        if item is None:\n",
    "            print(os.getpid(),\"received poison pill, exiting\")\n",
    "            return\n",
    "        predict(*item)\n",
    "    return\n",
    "\n",
    "# TODO(avjykmr2): Add batch support\n",
    "def generate_queries(model_name, query_generator, query_rate, num_samples, on_exception=lambda e : print(e), schedule=None):\n",
    "    \n",
    "    if schedule is None:\n",
    "        schedule = gen_poisson_query_schedule(query_rate, num_samples)\n",
    "    \n",
    "    try:\n",
    "#         the_pool = multiprocessing.Pool(4, query_worker,(query_queue,))\n",
    "        for number_of_queries, time_to_sleep in schedule:\n",
    "            if number_of_queries == 0:\n",
    "                time.sleep(time_to_sleep / 1000)\n",
    "            else:\n",
    "                for i in xrange(number_of_queries):\n",
    "                    new_query = query_generator.next()\n",
    "                    item = (clipper_conn.get_query_addr(), model_name, new_query)\n",
    "#                     query_queue.put(item)\n",
    "                    predict(*item)\n",
    "    except StopIteration as stopEx:\n",
    "        return\n",
    "    except Exception as e:\n",
    "        on_exception(e)\n",
    "        clipper_conn.stop_all()\n",
    "\n",
    "def feature_sum(xs):\n",
    "    return [str(sum(x)) for x in xs]\n",
    "\n",
    "# Stop Clipper on Ctrl-C\n",
    "def signal_handler(signal, frame):\n",
    "    print(\"Stopping Clipper...\")\n",
    "    clipper_conn = ClipperConnection(DockerContainerManager())\n",
    "    clipper_conn.stop_all()\n",
    "    sys.exit(0)\n",
    "\n",
    "def run(maxSamples):\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    clipper_conn.start_clipper()\n",
    "    python_deployer.create_endpoint(clipper_conn, \"simple-example\", \"doubles\",\n",
    "                                    feature_sum)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # fill args\n",
    "    generate_queries()\n",
    "    \n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
